{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJRylJhgn0w4",
        "outputId": "c781e0eb-a1d3-45de-9737-665fe03f844b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best R² Score: 0.9918\n",
            "Best Learning Rate: 0.01\n",
            "Best Regularization Parameter: 1e-15\n",
            "Best Weights: [0.17040012 0.51783856 0.72342713 0.60211981 0.79585619 0.96123597\n",
            " 1.07377076]\n"
          ]
        }
      ],
      "source": [
        "# Question ->1\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "np.random.seed(42)\n",
        "num_samples = 1000\n",
        "x1 = np.random.rand(num_samples)\n",
        "x2 = 2 * x1 + np.random.normal(0, 0.1, num_samples)\n",
        "x3 = 3 * x1 + np.random.normal(0, 0.1, num_samples)\n",
        "x4 = 4 * x1 + np.random.normal(0, 0.1, num_samples)\n",
        "x5 = 5 * x1 + np.random.normal(0, 0.1, num_samples)\n",
        "x6 = 6 * x1 + np.random.normal(0, 0.1, num_samples)\n",
        "x7 = 7 * x1 + np.random.normal(0, 0.1, num_samples)\n",
        "\n",
        "y = 10 * x1 + 2 * x2 + 3 * x3 + np.random.normal(0, 0.5, num_samples)\n",
        "\n",
        "data = pd.DataFrame({'x1': x1, 'x2': x2, 'x3': x3, 'x4': x4, 'x5': x5,\n",
        "                     'x6': x6, 'x7': x7, 'y': y})\n",
        "\n",
        "\n",
        "def ridge_regression(X, y, learning_rate, regularization_param, num_iterations):\n",
        "    m, n = X.shape\n",
        "    weights = np.zeros(n)\n",
        "    costs = []\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        y_pred = X @ weights\n",
        "        error = y_pred - y\n",
        "        cost = (1 / (2 * m)) * (np.sum(error ** 2) + regularization_param * np.sum(weights ** 2))\n",
        "        costs.append(cost)\n",
        "\n",
        "        gradient = (1 / m) * (X.T @ error) + (regularization_param / m) * weights\n",
        "        weights -= learning_rate * gradient\n",
        "\n",
        "\n",
        "        if np.isnan(cost) or np.isinf(cost):\n",
        "            break\n",
        "\n",
        "    return weights, costs\n",
        "\n",
        "\n",
        "learning_rates = [0.00001, 0.0001, 0.001, 0.01]\n",
        "regularization_params = [10**-15, 10**-10, 10**-5, 10**-3, 0, 1, 10, 20]\n",
        "num_iterations = 1000\n",
        "\n",
        "best_r2 = -np.inf\n",
        "best_weights = None\n",
        "best_lr = None\n",
        "best_reg = None\n",
        "\n",
        "X = data.drop('y', axis=1).values\n",
        "y = data['y'].values\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for reg in regularization_params:\n",
        "        weights, _ = ridge_regression(X, y, lr, reg, num_iterations)\n",
        "        y_pred = X @ weights\n",
        "        r2 = r2_score(y, y_pred)\n",
        "\n",
        "        if r2 > best_r2:\n",
        "            best_r2 = r2\n",
        "            best_weights = weights\n",
        "            best_lr = lr\n",
        "            best_reg = reg\n",
        "\n",
        "print(f\"Best R² Score: {best_r2:.4f}\")\n",
        "print(f\"Best Learning Rate: {best_lr}\")\n",
        "print(f\"Best Regularization Parameter: {best_reg}\")\n",
        "print(f\"Best Weights: {best_weights}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question ->2\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = pd.read_csv('/content/Hitters.csv')\n",
        "\n",
        "data = data.dropna()\n",
        "data = pd.get_dummies(data, drop_first=True)\n",
        "\n",
        "X = data.drop(columns=['Salary'])\n",
        "y = data['Salary']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "ridge_model = Ridge(alpha=0.5748)\n",
        "ridge_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "lasso_model = Lasso(alpha=0.5748)\n",
        "lasso_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_linear = linear_model.predict(X_test_scaled)\n",
        "y_pred_ridge = ridge_model.predict(X_test_scaled)\n",
        "y_pred_lasso = lasso_model.predict(X_test_scaled)\n",
        "\n",
        "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
        "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
        "\n",
        "print(\"Mean Squared Error for Linear Regression:\", mse_linear)\n",
        "print(\"Mean Squared Error for Ridge Regression:\", mse_ridge)\n",
        "print(\"Mean Squared Error for Lasso Regression:\", mse_lasso)\n",
        "\n",
        "if mse_linear < mse_ridge and mse_linear < mse_lasso:\n",
        "    print(\"Linear Regression performs the best.\")\n",
        "elif mse_ridge < mse_linear and mse_ridge < mse_lasso:\n",
        "    print(\"Ridge Regression performs the best.\")\n",
        "else:\n",
        "    print(\"Lasso Regression performs the best.\")\n"
      ],
      "metadata": {
        "id": "hEggJYCxoxU6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fffabf05-90d3-4dd1-b2b2-761797c359c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error for Linear Regression: 128284.34549672354\n",
            "Mean Squared Error for Ridge Regression: 126603.90264424692\n",
            "Mean Squared Error for Lasso Regression: 126739.56899132291\n",
            "Ridge Regression performs the best.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.185e+04, tolerance: 4.367e+03\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question->3\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "ridge_cv = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5)\n",
        "ridge_cv.fit(X_train_scaled, y_train)\n",
        "y_pred_ridge = ridge_cv.predict(X_test_scaled)\n",
        "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "\n",
        "lasso_cv = LassoCV(alphas=[0.1, 1.0, 10.0], cv=5)\n",
        "lasso_cv.fit(X_train_scaled, y_train)\n",
        "y_pred_lasso = lasso_cv.predict(X_test_scaled)\n",
        "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
        "\n",
        "print(\"Mean Squared Error for RidgeCV:\", mse_ridge)\n",
        "print(\"Mean Squared Error for LassoCV:\", mse_lasso)\n",
        "\n",
        "if mse_ridge < mse_lasso:\n",
        "    print(\"RidgeCV performs the best.\")\n",
        "else:\n",
        "    print(\"LassoCV performs the best.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xd7YmGlpzi3R",
        "outputId": "521b052d-cdf5-4972-8fcb-ba7df6040a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error for RidgeCV: 24.312903830491614\n",
            "Mean Squared Error for LassoCV: 25.656739367167678\n",
            "RidgeCV performs the best.\n"
          ]
        }
      ]
    }
  ]
}